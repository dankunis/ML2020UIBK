{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2020 Course Projects\n",
    "\n",
    "## Project Schedule\n",
    "\n",
    "In this project, you will solve a real-life problem with a dataset. The project will be separated into two phases:\n",
    "\n",
    "27th May - 9th June: We will give you a training set with target values and a testing set without target. You predict the target of the testing set by trying different machine learning models and submit your best result to us and we will evaluate your results first time at the end of phase 1.\n",
    "\n",
    "10th June - 24th June: Students stand high in the leader board will briefly explain  their submission in a proseminar. We will also release some general advice to improve the result. You try to improve your prediction and submit final results in the end. We will again ask random group to present and show their implementation.\n",
    "The project shall be finished by a team of two people. Please find your teammate and REGISTER via [here](https://docs.google.com/forms/d/e/1FAIpQLSf4uAQwBkTbN12E0akQdxfXLgUQLObAVDRjqJHcNAUFwvRTsg/alreadyresponded).\n",
    "\n",
    "The submission and evaluation is processed by [Kaggle](https://www.kaggle.com/t/426d97d4138b49b2802c2ee0461a18ac).  In order to submit, you need to create an account, please use your team name in the `team tag` on the [kaggle page](https://www.kaggle.com/t/426d97d4138b49b2802c2ee0461a18ac). Two people can submit as a team in Kaggle.\n",
    "\n",
    "You can submit and test your result on the test set 2 times a day, you will be able to upload your predicted value in a CSV file and your result will be shown on a leaderboard. We collect data for grading at 22:00 on the **last day of each phase**. Please secure your best results before this time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Car insurance companies are always trying to come up with a fair insurance plan for customers. They would like to offer a lower price to the careful and safe driver while the careless drivers who file claims in the past will pay more. In addition, more safe drivers mean that the company will spend less in operation. However, for new customers, it is difficult for the company to know who the safe driver is. As a result, if a company offers a low price, it bears a high risk of cost. If not, the company loses competitiveness and encourage new customers to choose its competitors.\n",
    "\n",
    "\n",
    "Your task is to create a machine learning model to mitigate this problem by identifying the safe drivers in new customers based on their profiles. The company then offers them a low price to boost safe customer acquirement and reduce risks of costs. We provide you with a dataset (train_set.csv) regarding the profile (columns starting with ps_*) of customers. You will be asked to predict whether a customer will file a claim (`target`) in the next year with the test_set.csv \n",
    "\n",
    "~~You can find the dataset in the `data/final-project-data` folders in the jupyter hub.~~ We also upload dataset to Kaggle and will test your result and offer you a leaderboard in Kaggle. Please find them under the Data tag on the following page:\n",
    "\n",
    "https://www.kaggle.com/t/426d97d4138b49b2802c2ee0461a18ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: 26th May - 9th June\n",
    "\n",
    "### Data Description\n",
    "\n",
    "In order to take a look at the data, you can use the `describe()` method. As you can see in the result, each row has a unique `id`. `Target` $\\in \\{0, 1\\}$ is whether a user will file a claim in his insurance period. The rest of the 57 columns are features regarding customers' profiles. You might also notice that some of the features have minimum values of `-1`. This indicates that the actual value is missing or inaccessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.18.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: matplotlib==3.1.3 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: pandas==1.0.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: tqdm==4.43.0 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.43.0)\n",
      "Requirement already satisfied: scikit_learn==0.22.2.post1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
      "Requirement already satisfied: catboost==0.23.2 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
      "Requirement already satisfied: skorch==0.8.0 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: imbalanced-learn==0.6.2 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: missingno in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\domi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (2.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3->-r requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from pandas==1.0.1->-r requirements.txt (line 4)) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from scikit_learn==0.22.2.post1->-r requirements.txt (line 6)) (0.15.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\domi\\anaconda3\\lib\\site-packages (from catboost==0.23.2->-r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\domi\\anaconda3\\lib\\site-packages (from catboost==0.23.2->-r requirements.txt (line 7)) (4.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\domi\\anaconda3\\lib\\site-packages (from catboost==0.23.2->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from skorch==0.8.0->-r requirements.txt (line 8)) (0.8.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\domi\\anaconda3\\lib\\site-packages (from missingno->-r requirements.txt (line 10)) (0.10.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\domi\\anaconda3\\lib\\site-packages (from plotly->catboost==0.23.2->-r requirements.txt (line 7)) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "# Quick load dataset and check\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n",
    "if not running_local:\n",
    "    path = \"/data/final-project-dataset/\"\n",
    "else:\n",
    "    path = \"./data/\"\n",
    "    !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(path, \"train_set.csv\")\n",
    "data_train = pd.read_csv(filename)\n",
    "filename = path + \"test_set.csv\"\n",
    "data_test = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix, e.g. `ind` and `calc`, indicate the feature belongs to similiar groupings. The postfix `bin` indicates binary features and `cat` indicates categorical features. The features without postfix are ordinal or continuous. Similarly, you can check the statistics for testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from contracts import contract\n",
    "import sklearn\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_02_cat\t0.03\n",
      "ps_ind_04_cat\t0.01\n",
      "ps_ind_05_cat\t0.98\n",
      "ps_reg_03\t18.13\n",
      "ps_car_01_cat\t0.02\n",
      "ps_car_02_cat\t0.00\n",
      "ps_car_03_cat\t69.07\n",
      "ps_car_05_cat\t44.77\n",
      "ps_car_07_cat\t1.91\n",
      "ps_car_09_cat\t0.09\n",
      "ps_car_11\t0.00\n",
      "ps_car_12\t0.00\n",
      "ps_car_14\t7.15\n"
     ]
    }
   ],
   "source": [
    "# view the missing columns\n",
    "missing_col = {}\n",
    "for col in data_train.columns:\n",
    "    counter = len(data_train[data_train[col] == -1])\n",
    "    if counter > 0:\n",
    "        missing_col[col] = counter / len(data_train) * 100\n",
    "        print('{}\\t{:.2f}'.format(col, missing_col[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape[1])\n",
    "data_train.drop(columns=[col for col, val in missing_col.items() if val >= 10], inplace=True)\n",
    "data_test.drop(columns=[col for col, val in missing_col.items() if val >= 10], inplace=True)\n",
    "print(data_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_car_03_cat\t-> ps_car_03_bin\n",
      "ps_car_05_cat\t-> ps_car_05_bin\n",
      "ps_car_03_cat\t-> ps_car_03_bin\n",
      "ps_car_05_cat\t-> ps_car_05_bin\n"
     ]
    }
   ],
   "source": [
    "## not used in 0.52\n",
    "data_train.drop(columns=[col for col, val in missing_col.items() if val >= 10 and 'cat' not in col], inplace=True)\n",
    "data_test.drop(columns=[col for col, val in missing_col.items() if val >= 10 and 'cat' not in col], inplace=True)\n",
    "\n",
    "# transform cat to bin and fill the rest with median values\n",
    "for df in [data_train, data_test]:\n",
    "    for col, val in missing_col.items():\n",
    "        if val >= 10:\n",
    "            if 'cat' not in col:\n",
    "                continue\n",
    "            df.loc[df[col] != -1, col] = 1\n",
    "            df[col].replace(-1, 0, inplace=True)\n",
    "            df.rename(columns={col: col.replace('cat', 'bin')}, inplace=True)\n",
    "            print('{}\\t-> {}'.format(col, col.replace('cat', 'bin')))\n",
    "            continue\n",
    "        median = df[df[col] != -1][col].median()\n",
    "        df[col].replace(-1, median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']\n"
     ]
    }
   ],
   "source": [
    "## not used in 0.52\n",
    "\n",
    "# drop features with '_calc_' in feature names\n",
    "feature_calc = list(data_train.columns[data_train.columns.str.contains('_calc_')])\n",
    "print(feature_calc)\n",
    "\n",
    "data_train = data_train.drop(feature_calc, axis = 1)\n",
    "data_test = data_test.drop(feature_calc, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9091674540493707\n",
      "0.9346317670937678\n",
      "0.9456814518181943\n",
      "0.9527356451329673\n",
      "0.9590318073817083\n",
      "0.9651454433117876\n",
      "0.9704684099774006\n",
      "0.9750092156046967\n",
      "0.9786123544960751\n",
      "0.9815372140898345\n",
      "0.9839306292500312\n",
      "0.9857103493575371\n",
      "0.9873797389018276\n",
      "0.9888929768515148\n",
      "0.9903795713294008\n",
      "0.9916775939394392\n",
      "0.9928864841620509\n",
      "0.9939627521893217\n",
      "0.9950044174600585\n",
      "0.9957494158101426\n",
      "0.9963452126348805\n",
      "0.9967859612629872\n",
      "0.9971785534306211\n",
      "0.9974400778524953\n",
      "0.9976603968153036\n",
      "0.9978666045810682\n",
      "0.9980616565224008\n",
      "0.9982512867590098\n",
      "0.998427857827909\n",
      "0.9985989137900184\n",
      "0.9987571553819683\n",
      "0.9988907972569371\n",
      "0.999018243536765\n",
      "54\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "## Select target and features\n",
    "fea_col = data_train.columns[2:]\n",
    "data_Y = data_train['target']\n",
    "data_X = data_train[fea_col]\n",
    "\n",
    "## Select target and features\n",
    "fea_col = data_train.columns[2:]\n",
    "data_Y = data_train['target']\n",
    "data_X = data_train[fea_col]\n",
    "\n",
    "percent_data = 0.999\n",
    "\n",
    "## Get components that explain over 99.9% of data\n",
    "pca = PCA(0, svd_solver='full')\n",
    "for i in range(1,data_X.shape[1]):\n",
    "    pca = PCA(i, svd_solver='full')\n",
    "    pca.fit_transform(data_X)\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    if pca.explained_variance_ratio_.sum() > percent_data:\n",
    "        break\n",
    "\n",
    "## Transfrom data\n",
    "print(data_X.shape[1])\n",
    "data_X = pca.transform(data_X)\n",
    "print(data_X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# TODO\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit_transform(train.drop(['target'], axis=1))\n",
    "data_train = preprocessing.normalize(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "### Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n",
    "\n",
    "# try using class weight instead\n",
    "#_, counts = np.unique(y_train, return_counts=True)\n",
    "#weights =  counts[0] / counts\n",
    "          \n",
    "#print(weights)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# # Random Oversampling\n",
    "# over = RandomOverSampler(sampling_strategy=1)\n",
    "# # fit and apply the transform\n",
    "# x_train, y_train = over.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_pytorch(net, inputs, labels, optimizer, criterion, iterations=1000):\n",
    "    \"\"\"\n",
    "    :param net: the neural network object\n",
    "    :param inputs: numpy array of training data values\n",
    "    :param labels: numpy array of training data labels \n",
    "    :param optimizer: PyTorch optimizer instance\n",
    "    :param criterion: PyTorch loss function\n",
    "    :param iterations: number of training steps\n",
    "    \"\"\"\n",
    "    net.train()  # Before training, set the network to training mode\n",
    "\n",
    "    for iter in trange(iterations):  # loop over the dataset multiple times\n",
    "        \n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        # Convert to tensors if data is in the form of numpy arrays\n",
    "        if not torch.is_tensor(inputs):\n",
    "            inputs = torch.from_numpy(inputs.astype(np.float32)) \n",
    "            \n",
    "        if not torch.is_tensor(labels):\n",
    "            labels = torch.from_numpy(labels.astype(np.float32))\n",
    "\n",
    "        # 1. Reset gradients\n",
    "        optimizer.zero_grad()  \n",
    "        # 2. Forward\n",
    "        outputs = net(inputs)\n",
    "        # 3. Compute the loss\n",
    "        loss = criterion(outputs.reshape(-1), labels)\n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        # 5. Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pytorch(net, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Function for producing network predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    X = torch.from_numpy(X.astype(np.float32))\n",
    "    logits = net(X)\n",
    "    predictions = torch.sigmoid(logits) > threshold\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "@contract(Y_pred='array[Mx1],M>0',\n",
    "          Y='array[Mx1],M>0',\n",
    "          returns='float,>=0.0,<=1.0')\n",
    "def calc_accuracy(Y_pred, Y):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions against the true labels\n",
    "    (What percent of the predicted labels Y_pred matches the true labels in Y)\n",
    "    \n",
    "    param: Y_pred: Predictions of our model (numpy array of shape [m,1] containing 0s and 1s)\n",
    "    param: Y: Target labels (numpy array of shape [m,output_dim])\n",
    "    \n",
    "    returns: accuracy (float between 0.0 and 1.0)  \n",
    "    \"\"\"\n",
    "    \n",
    "    #accuracy = float(np.dot(Y.T,Y_pred) + np.dot((1-Y).T,1-Y_pred))/float(Y.size)\n",
    "    \n",
    "    return f1_score(Y, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input x keeping the batch dimension the same\n",
    "        x = x.reshape(-1, self.input_size)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = self.fc3(x)          \n",
    "\n",
    "        return x  # Return x (logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "MAX_ITERATIONS = 100\n",
    "INPUT_SIZE = x_train.shape[1]\n",
    "HIDDEN_SIZE = 12 # empirical rule ~ mean of the neurons in the input and output layers\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, DROPOUT)\n",
    "\n",
    "# Define the loss criterion and the training algorithm\n",
    "criterion = nn.BCEWithLogitsLoss()  # Be careful, use binary cross entropy for binary, CrossEntropy for Multi-class\n",
    "# optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2448358150df473b995487f3d7b441e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.40398, Test F1 score: 0.52120, threshold: 0.5800\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c6e398adf44e10ac988a8153fa175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.41243, Test F1 score: 0.51776, threshold: 0.5300\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b10de5e1dc48f48a14574e1ddbb1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.42709, Test F1 score: 0.51903, threshold: 0.5600\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dd64b3adf14d719d92d3e948287fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.40077, Test F1 score: 0.52112, threshold: 0.5800\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fa493f5aba453e869b361707dc7a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.40970, Test F1 score: 0.51770, threshold: 0.5900\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17701d65eb67449aa104cf3223d4c78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.40599, Test F1 score: 0.52064, threshold: 0.5600\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2474f0110d46649863ac718d2a9500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.41439, Test F1 score: 0.52065, threshold: 0.5600\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a712b3862b8946b5b9b3080f5cc0fbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Training\n",
      "Train F1 score: 0.40686, Test F1 score: 0.51708, threshold: 0.5900\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "\n",
    "# Test different hidden sizes\n",
    "net_list = []\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "\n",
    "for i in [16,17,18,19]:\n",
    "    HIDDEN_SIZE = i\n",
    "    for j in [0.5,0.6]:  \n",
    "        DROPOUT = j\n",
    "        net = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, DROPOUT)\n",
    "        #criterion = nn.BCEWithLogitsLoss()  # Be careful, use binary cross entropy for binary, CrossEntropy for Multi-class\n",
    "        #optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "        train_neural_network_pytorch(net, x_train, y_train, optimizer, criterion, MAX_ITERATIONS)\n",
    "        \n",
    "        # i tried to tune the threshold parameter\n",
    "        scores = [f1_score(np.array(y_val).reshape(-1,1), predict_pytorch(net, np.array(x_val), threshold=t).data.numpy(), average = 'macro') for t in thresholds]\n",
    "        ix = argmax(scores)\n",
    "        \n",
    "        train_macro_f = f1_score(np.array(y_train).reshape(-1,1), predict_pytorch(net, np.array(x_train), threshold=thresholds[ix]).data.numpy(), average = 'macro')\n",
    "        test_macro_f = scores[ix]\n",
    "        \n",
    "        net_list.append((net,test_macro_f,thresholds[ix]))\n",
    "        print(f\"Train F1 score: {train_macro_f:.5f}, Test F1 score: {test_macro_f:.5f}, threshold: {thresholds[ix]:.4f}\")\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58\n",
      "Train accuracy: 0.40398, Test accuracy: 0.52120\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "# Take network with best accuracy\n",
    "net = max(net_list, key=operator.itemgetter(1))[0]\n",
    "threshold = max(net_list, key=operator.itemgetter(1))[2]\n",
    "print(threshold)\n",
    "\n",
    "train_acc = calc_accuracy(predict_pytorch(net, np.array(x_train), threshold=threshold).data.numpy(), np.array(y_train).reshape(-1,1))\n",
    "test_acc = calc_accuracy(predict_pytorch(net, np.array(x_val), threshold=threshold).data.numpy(), np.array(y_val).reshape(-1,1))\n",
    "print(f\"Train accuracy: {train_acc:.5f}, Test accuracy: {test_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Please only submit the csv files with predicted outcome with its id and target [here](https://www.kaggle.com/t/b3dc81e90d32436d93d2b509c98d0d71). Your column should only contain `0` and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_X = data_test.drop(columns=['id'])\n",
    "y_target = np.array(predict_pytorch(net, np.array(data_test_X), threshold=threshold)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = pd.DataFrame(data_test['id'].copy())\n",
    "data_out.insert(1, \"target\", y_target, True) \n",
    "data_out.to_csv('./data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148795</th>\n",
       "      <td>248795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148796</th>\n",
       "      <td>248796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148797</th>\n",
       "      <td>248797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148798</th>\n",
       "      <td>248798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148799</th>\n",
       "      <td>248799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148800 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  target\n",
       "0       100000       0\n",
       "1       100001       0\n",
       "2       100002       0\n",
       "3       100003       0\n",
       "4       100004       0\n",
       "...        ...     ...\n",
       "148795  248795       0\n",
       "148796  248796       0\n",
       "148797  248797       0\n",
       "148798  248798       0\n",
       "148799  248799       0\n",
       "\n",
       "[148800 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6894"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_out['target']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
